# Installing localGPT
# make a new conda environment
> conda create -n test python=3.9.0 ipython

# install torch with cuda
> pip install torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

> pip install -r requirements.txt
# needed to downgrade to huggingface_hub==0.24.7 as cached_download has been deprecated
# may need to change versions/patches, see conda_working_cuda.txt for working state

> CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=cuBLAS" FORCE_CMAKE=1 pip install llama-cpp-python

> pip install langchain==0.0.348

# Running localGPT
> conda activate /work/atif/localGPT-env/

# then follow the steps in localGPT's README
# FastCaloSim related documents are in /work/atif/localGPT-env/FCS-KB

# For Treesitter parser
pip install tree-sitter==0.20.4 tree-sitter-languages==1.10.2





# python run_localGPT.py --model_type=llama3 --temperature=0.01
#
# Installing Doxygen and graphviz
# TODO
# Edit Doxyfile with name of PROJECT_NAME and INPUT
# 
# Install websocat
git clone https://github.com/vi/websocat.git
cd websocat/
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
ls $HOME/.cargo/bin/cargo
export PKG_CONFIG_PATH=/home/atif/packages/openssl-1.1.1l/lib/pkgconfig/
/home/atif/.cargo/bin/cargo build --release --features=ssl
export LD_LIBRARY_PATH=/home/atif/packages/openssl-1.1.1l/lib/:$LD_LIBRARY_PATH
# Launch websocat WebSocket for terminal
/home/atif/packages/websocat/target/release/websocat -E -t ws-l:127.0.0.1:4461 exec:/bin/bash
#
# Edit doxyRAG/mywebsite.html with path to HEP_directory/doxygen/html
#
# cd doxyRAG and launch up Python server
python3 -m http.server 4461 --bind 0.0.0.0

# Port Forward
ssh -L 4461:localhost:4461 atif@dahlia.csi.bnl.gov

# Open http://localhost:4461/annotated.html on your browser
#
#
#
